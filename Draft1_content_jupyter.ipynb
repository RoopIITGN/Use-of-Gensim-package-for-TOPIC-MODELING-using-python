{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                 Use of Gensim package in python for TOPIC MODELING : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's describe topic modeling and its need in short. The amount of data available on the internet is growing every second. But all of this huge data is not relevant to us for any particular work. So we need a sort of a tool which would help us to understand the main themes or topics of a document prior reading it. So, the technique should be able to do so without knowing the topic/theme of the document. We might be also interested to see the dynamic change in the themes with time in a document.  LDA(Latent Dirichlet Allocation) is a generative model which serves this purpose. We will see the application part of it. For a given corpus we will see how we can extract the hidden topics with the help of Gensim package in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites are: You should have nltk stopwords, spacy model, gensim, pyLDAvis, and 'en' library of spacy package downloaded in your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in terminal:\n",
    "(if it doesn't work try some other methods to install these ) : \n",
    "\"pip3 install spacy\" , \"pip3 install gensim\" , \"pip3 install pyLDAvis\" , \"python3 -m spacy download en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/s18210071/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this in python interpreter\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# For Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy as spc\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Stopwords: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_list = stopwords.words('english')\n",
    "# we can extend the stopwords if needed according to our need: \n",
    "# stop_words_list.extend(['word1', 'word2', 'word3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to tokenize sentences into list of words, ignoring the punctuations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_word_converter(sentences):\n",
    "    for s in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(s), deacc=True))  # deacc=True ignores punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(words):\n",
    "    return [[word for word in simple_preprocess(str(element)) if word not in stop_words_list] for element in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to make Biagrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_maker(words):\n",
    "    return [bigram_mod[element] for element in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to make Trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_maker(words):\n",
    "    return [trigram_mod[bigram_mod[element]] for element in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for the lemmatization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    text_out = []\n",
    "    for sen in text:\n",
    "        doct = nlp(\" \".join(sen)) \n",
    "        text_out.append([token.lemma_ for token in doct if token.pos_ in allowed_postags])\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset or document: (Here we are importing a science fiction book's data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"aliensfaq.txt\"\n",
    "f=open(filename,'r')\n",
    "file1=f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the whole file into list of senteces and remove the non english words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sent=nltk.sent_tokenize(file1)\n",
    "english_words=set(nltk.corpus.words.words())\n",
    "english_inputs=[]\n",
    "for i in list_sent:\n",
    "    j=\" \".join(w for w in nltk.wordpunct_tokenize(i) if w.lower() in english_words or not w.isalpha())\n",
    "    english_inputs.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of regular expression let's clean the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words which includes @ from the list:\n",
    "english_inputs=[re.sub('\\S*@\\S*\\s?', '', sent) for sent in english_inputs]\n",
    "#remove new lines from the list:\n",
    "english_inputs=[re.sub('\\s+', ' ', sent) for sent in english_inputs]\n",
    "# Remove distracting single quotes from the list:\n",
    "english_inputs=[re.sub(\"\\'\", \"\", sent) for sent in english_inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this list of sentences to list of words, and then build the bigram and trigrmas, and remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s18210071/.local/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "input_words=list(sent_word_converter(english_inputs))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigrams = gensim.models.Phrases(input_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigrams = gensim.models.Phrases(bigrams[input_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigrams)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigrams)\n",
    "non_stopwords = delete_stopwords(input_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the Bigrams and Trigrams lemmatize them and them make the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "bigram_words = bigram_maker(non_stopwords)\n",
    "\n",
    "# Load spacy 'en' model, we only need tagger component\n",
    "nlp = spc.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatize keeping only noun, adj, vb, adv\n",
    "lemmatized_words = lemmatize(bigram_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(lemmatized_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = lemmatized_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Building LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=5, \n",
    "                                        random_state=100,\n",
    "                                        update_every=1,\n",
    "                                        chunksize=100,\n",
    "                                        passes=10,\n",
    "                                        alpha='auto',\n",
    "                                        per_word_topics=True)\n",
    "# change the code and Print the Keyword in the 10 topics\n",
    "#pprint(lda_model.print_topics())\n",
    "topics=lda_model.print_topics()\n",
    "print(\"Top 5 topics are: \")\n",
    "ct=1\n",
    "for i in topics:\n",
    "    print(\"\\nTopic \",ct,\": \")\n",
    "    ct+=1\n",
    "    pprint(i[1])\n",
    "    \n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here I have taken help from theses site to understand how to use Gensim for topic modeling:\n",
    "https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html and https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
